{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Given a floor plan, detect different objects present the floor plan. The object can be door, windows, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANS: There are several methods to object detection:\n",
    " \n",
    "Labeling the objects using  contour/ bounding box: \n",
    "\n",
    "Object Detection using Template Matching which can be done without machine learning, using template image.\n",
    "\n",
    "Object detection using deep learning: \n",
    "     There are models available for doing object detection recognition in an image. For example, the most prominent ones are RCNN, fast RCNN, faster RCNN, mask RCNN, Yolo and SSD. However, most of the them are developed for natural images/Real life images. \n",
    "     \n",
    "    In this case, we are working on document images ex floor plans, where we need to detect objects like Windows, flowerpots, dining table, Bed, Computer, sofa, sink, etc.\n",
    "    \n",
    "    There are not many model available for the  documents type images.\n",
    "    \n",
    "    In order to apply deep learning, we need training image labelled with classes (in our case, icons used by architecture software in a floorplan).\n",
    "    \n",
    "    I tried to use existing  YOLO model ,and loaded  pretrained model and weight for document type, with darkflow implementation and  however facing import issue with Darflow (https://github.com/thtrieu/darkflow).\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the interest of time, have labeled the objects using contour only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Wall measurement \n",
    "#to get contour for SOLID walls, uncomment ->'img_gray=255-img_gray', to contur the rets of the object, \n",
    "#comment-->  img_gray=255-img_gray.\n",
    "#for SOLID Walls, min area =50~100 and max: 60,000.\n",
    "#for Rest of the object, min area = 1 and maximum area =7500 (excluding rooms open space detection, which have highest area)\n",
    "#****Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object.\n",
    "#Draw Contour: first argument is source image, second argument is the contours which should be passed as a Python list, \n",
    "#third argument is index of contours (useful when drawing individual contour. To draw all contours, pass -1) and \n",
    "#remaining arguments are color, thickness etc.\n",
    "    \n",
    "img = cv2.imread('floorplan.jpg') #This image is of size (351,555,3)\n",
    "img=cv2.resize(img,(900,500))\n",
    "img_gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #Converted to gray scale\n",
    "#img_gray=255-img_gray   #inverted to highlight solid lines in white\n",
    "gray = cv2.threshold(img_gray.copy(), 230,255, cv2.THRESH_BINARY)[1] #threshold applied to cleanout noise\n",
    "contours,hierarchy = cv2.findContours(gray,cv2.RETR_LIST ,cv2.CHAIN_APPROX_SIMPLE )\n",
    "\n",
    "#Calculat the boundary box of individual contour and store it in contourBBS\n",
    "contoursBBS = []\n",
    "for contour in contours:\n",
    "    [x, y, w, h] = cv2.boundingRect(contour)\n",
    "    contoursBBS.append([x, y, w, h, w*h])\n",
    "contoursBBS = np.array(contoursBBS)\n",
    "\n",
    "#Filter the boundry box based on width, height and area.\n",
    "#We can tune the box sized accordingly\n",
    "contourRects = []\n",
    "for c in contoursBBS:\n",
    "    [x, y, w, h, a] = c\n",
    "    if w < 1 or h <1 or a < 10 or a > 60000:    # \n",
    "        continue\n",
    "    contourRects.append(c)\n",
    "\n",
    "    #Get the center point of the bounding box\n",
    "for i, rect in enumerate(contourRects):\n",
    "    [x, y, w, h, a] = rect\n",
    "    topCenter, bottomCenter = (int((2*x + w)/2), int(y)), (int((2*x + w)/2), int(y+h))\n",
    "\n",
    "\n",
    "#draw the bounding box on the detected object\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    cv2.circle(img, topCenter, 2, (0, 0, 255), 2)\n",
    "    cv2.circle(img, bottomCenter, 2, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "cv2.imshow('img',img)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connected cmponents to get Number of objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats:  121\n",
      "Centroid:  121\n",
      "label:  500\n",
      "Unique Labels  121\n"
     ]
    }
   ],
   "source": [
    "#Using the connected component stats method to highlight the Walls/Number of object identified. Connected component stats returns, \n",
    "#labels with stats coordinates (x,y,w,h,a) and centroid info.\n",
    "\n",
    "#Color image is loaded so that color outlines can displayed\n",
    "img_color = cv2.imread('floorplan.jpg')   #canvas.jpg \n",
    "img_color=cv2.resize(img_color,(900,500))\n",
    "\n",
    "#Load the image, resize, change to gray and apply connected components\n",
    "img = cv2.imread('floorplan.jpg') #This image is of size (351,555,3)\n",
    "img=cv2.resize(img,(900,500))\n",
    "img_gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #Converted to gray scale\n",
    "#img_gray=255-img_gray   #inverted to highlight solid lines in white\n",
    "gray = cv2.threshold(img_gray.copy(), 235,255, cv2.THRESH_BINARY)[1]   #245,255,\n",
    "# Find the connected components on the floor\n",
    "nb_components, labels, stats, centroids = cv2.connectedComponentsWithStats(gray, connectivity=8)# takes in gray image, connectivity to lookout for corners and output image data type.\n",
    "\n",
    "#Threshold to filterout noises\n",
    "gap_threshold = 20\n",
    "\n",
    "#Create a empty list to collect identified objects\n",
    "Walls_object=[]\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "\n",
    "unique = np.unique(labels) #labels are in multitude, so we get only unique labels\n",
    "\n",
    "#Loop through the labels to highlight identified areas and gets stats(coordinates) f\n",
    "for i in range(0, len(unique)):\n",
    "    component = labels == unique[i]  # enforce to select only unique label\n",
    "    stat = stats[i]    # get the stats and centroid for that label\n",
    "    centroid= centroids[i]\n",
    "    if img_color[component].sum() == 0 or np.count_nonzero(component) < gap_threshold:   #filter out the undesirable noises\n",
    "        color = 0\n",
    "    else:\n",
    "        Walls_object.append([i, component,stat, centroid])  # collect the labels and stats \n",
    "        color = np.random.randint(0, 255, size=3)\n",
    "    img_color[component] = color    #hightlight label/components on image \n",
    "    text=\"Label_\" +str(i)\n",
    "    x,y = centroid\n",
    "    cv2.putText(img_color, text, (x.astype(int), y.astype(int)),font, 0.4, (250, 0,0)) #print Labels\n",
    "\n",
    "#show image\n",
    "cv2.imshow('img',img_color)   \n",
    "cv2.waitKey()\n",
    "print(\"Stats: \",len(stats))\n",
    "print(\"Centroid: \",len(centroids))\n",
    "print(\"label: \",len(labels))\n",
    "print(\"Unique Labels \", len(unique))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
